{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nlp-1-final.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8B72CloGwDLk"},"source":["# Natural Language Processing - Exercise 1 (Problem 4)\n","\n","## Authors\n","* Charteros Eleftherios ([l.harteros@gmail.com](mailto:l.harteros@gmail.com))\n","* Kotitsas Sotirios ([sotiriskot9@gmail.com](mailto:sotiriskot9@gmail.com))\n","* Stavropoulos Petros ([pstav1993@gmail.com](mailto:pstav1993@gmail.com))\n","* Xenouleas Efstratios ([stratosxen@gmail.com](mailto:stratosxen@gmail.com))"]},{"cell_type":"code","metadata":{"id":"vGDo9i5mT4mj","executionInfo":{"status":"ok","timestamp":1612641604669,"user_tz":-120,"elapsed":2260,"user":{"displayName":"Stratos Xenouleas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR90vC-94SBOfz6YtZudAly36OJWSWrSktd543Xg=s64","userId":"04399389374450208758"}}},"source":["# Import everything\n","\n","import nltk\n","import string\n","import math\n","import copy\n","import random\n","from nltk.corpus import gutenberg, brown, stopwords\n","from nltk import ngrams\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.tokenize.treebank import TreebankWordDetokenizer\n","from collections import Counter\n","from nltk.util import ngrams, pad_sequence\n","from pprint import pprint\n","from tqdm import tqdm\n","import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"tHaD_U8pUJwi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612641737456,"user_tz":-120,"elapsed":1773,"user":{"displayName":"Stratos Xenouleas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR90vC-94SBOfz6YtZudAly36OJWSWrSktd543Xg=s64","userId":"04399389374450208758"}},"outputId":"d1ee4b03-0004-4deb-fa3b-2add95d93c9b"},"source":["# Set seed for random\n","random.seed(1)\n","np.random.seed(1)\n","\n","# ALPHA is for a-smoothing\n","# N is the minimum frequency for a word to include it in the vocab\n","ALPHA = 0.01\n","N = 10\n","\n","# Download corpuses, punctuation and stopwords\n","nltk.download('gutenberg')\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Print the nltk version\n","print(nltk.__version__)\n","print(gutenberg.fileids())"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/gutenberg.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","3.2.5\n","['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rU8xpTHqVJ-5"},"source":["# Part A"]},{"cell_type":"code","metadata":{"id":"4weBG_y6UR0M","executionInfo":{"status":"ok","timestamp":1612641737720,"user_tz":-120,"elapsed":1316,"user":{"displayName":"Stratos Xenouleas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR90vC-94SBOfz6YtZudAly36OJWSWrSktd543Xg=s64","userId":"04399389374450208758"}}},"source":["# Get a specific text from gutenberg corpus\n","text = gutenberg.raw('austen-emma.txt')\n","\n","# Or get all the gutenberg texts (for testing)\n","# text = ''\n","# for fid in gutenberg.fileids():\n","#   text += gutenberg.raw(fid) + ' '\n","# text = text[:-1]\n","\n","# Remove some characters from text\n","text = text.translate(str.maketrans('', '', \"()[]/:,;-_\\\"'*\"))"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"hBw3lEtYUS0V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612641739117,"user_tz":-120,"elapsed":2422,"user":{"displayName":"Stratos Xenouleas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR90vC-94SBOfz6YtZudAly36OJWSWrSktd543Xg=s64","userId":"04399389374450208758"}},"outputId":"fd4aa360-ae3e-4490-d435-bc63f8c5ce64"},"source":["# Perform sentence splitting/tokenization to the text\n","sentences = sent_tokenize(text)\n","\n","# Perform word tokenization for each sentence and add the tokenized words to the list\n","sentences_tokenized = []\n","for sent in tqdm(sentences):\n","    tokens = word_tokenize(sent.lower())\n","    sentences_tokenized.append(tokens)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["100%|██████████| 7738/7738 [00:01<00:00, 5670.35it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"KGc09I7gUV4O","executionInfo":{"status":"ok","timestamp":1612641739374,"user_tz":-120,"elapsed":2126,"user":{"displayName":"Stratos Xenouleas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR90vC-94SBOfz6YtZudAly36OJWSWrSktd543Xg=s64","userId":"04399389374450208758"}}},"source":["# Split train - val - test\n","# Use 80% of the sentences as a training set\n","\n","train_size = int(len(sentences_tokenized) * 0.8)\n","test_size = train_size + int((len(sentences_tokenized) - train_size) / 2)\n","\n","train_set = sentences_tokenized[:train_size]\n","val_set = sentences_tokenized[train_size:test_size]\n","test_set = sentences_tokenized[test_size:]"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"YLsoPS7BUZZA","executionInfo":{"status":"ok","timestamp":1612641739375,"user_tz":-120,"elapsed":1622,"user":{"displayName":"Stratos Xenouleas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR90vC-94SBOfz6YtZudAly36OJWSWrSktd543Xg=s64","userId":"04399389374450208758"}}},"source":["# Flatten the sentences of the training set into words\n","words = [item for sublist in train_set for item in sublist]\n","\n","# Find the frequency of words in the train set\n","freq = Counter()\n","freq.update(words)\n","\n","# Create the vocab from the train set for words with frequency more than the specified\n","vocab = set()\n","for sent in train_set:\n","    vocab.update([word for word in sent if freq[word] >= N])\n","vocab.add('<UNK>')\n","vocab.add('<start>')\n","vocab.add('<start1>')\n","vocab.add('<start2>')\n","vocab.add('<end>')\n","\n","# Create a dictionary from the set to use as a vocab\n","vocab = {word: i for i, word in enumerate(vocab)}\n","inv_vocab = {vocab[word]: word for word in vocab}\n","\n","vocab_size = len(vocab)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"hvmhiR1BUcUS","executionInfo":{"status":"ok","timestamp":1612641739376,"user_tz":-120,"elapsed":896,"user":{"displayName":"Stratos Xenouleas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR90vC-94SBOfz6YtZudAly36OJWSWrSktd543Xg=s64","userId":"04399389374450208758"}}},"source":["# Replace unkown words in the sets\n","# (we could also use the defaultdict nltk module)\n","for i in range(len(train_set)):\n","    for j in range(len(train_set[i])):\n","        if train_set[i][j] not in vocab:\n","            train_set[i][j] = '<UNK>'\n","for i in range(len(val_set)):\n","    for j in range(len(val_set[i])):\n","        if val_set[i][j] not in vocab:\n","            val_set[i][j] = '<UNK>'\n","for i in range(len(test_set)):\n","    for j in range(len(test_set[i])):\n","        if test_set[i][j] not in vocab:\n","            test_set[i][j] = '<UNK>'"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"7mdRi2s9Uigt","executionInfo":{"status":"ok","timestamp":1612641741087,"user_tz":-120,"elapsed":866,"user":{"displayName":"Stratos Xenouleas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR90vC-94SBOfz6YtZudAly36OJWSWrSktd543Xg=s64","userId":"04399389374450208758"}}},"source":["# Count ngrams\n","unigram_counter = Counter()\n","bigram_counter = Counter()\n","trigram_counter = Counter()\n","\n","# For each sentence in the training set get the ngrams and update the according Counter\n","for sent in train_set:\n","    unigram_counter.update(sent)\n","    bigrams = ngrams(['<start>'] + sent + ['<end>'], 2)\n","    bigram_counter.update(bigrams)\n","    trigrams = ngrams(['<start1>'] + ['<start2>'] + sent + ['<end>'], 3)\n","    trigram_counter.update(trigrams)\n","\n","# Add the special start tokens to the counters, in order for them to be used by the next-order ngram models\n","unigram_counter['<start>'] = len(train_set)\n","bigram_counter[('<start1>', '<start2>')] = len(train_set)\n","\n","# Copy all the bigrams that start with the <start> token to bigrams that start with <start2> tokens in order\n","# to avoide zero probability on trigrams that start with the <start2> token\n","# ex. Calculating P(product| <start2>, the) = C(<start2>, the, product) / C(<start2>, the)\n","to_add = list()\n","for k in bigram_counter.keys():\n","    if k[0] == '<start>':\n","        to_add.append(('<start2>', k[1],  bigram_counter[k]))\n","for k in to_add:\n","    bigram_counter[(k[0], k[1])] = k[2]"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8lCPpVf8VUgd"},"source":["# Part B"]},{"cell_type":"code","metadata":{"id":"ggic7UfjUn6I","executionInfo":{"status":"ok","timestamp":1612641742867,"user_tz":-120,"elapsed":641,"user":{"displayName":"Stratos Xenouleas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR90vC-94SBOfz6YtZudAly36OJWSWrSktd543Xg=s64","userId":"04399389374450208758"}}},"source":["# Find the log probability of a sentence to appear on a language model using different ngram models\n","def predict(sent, ngram):\n","    if ngram not in [1, 2, 3]:\n","        print('Please choose either a unigram, bigram or trigram language model')\n","        return None\n","\n","    # Find the aggregated log probability of the sentence by adding the log probabilities\n","    # of each ngram in the sentence\n","    prob = 0\n","    if ngram == 1:\n","        # Sum of all unigrams (count of all words in the train set)\n","        C = sum(unigram_counter.values())\n","        for i in range(0, len(sent) - (ngram - 1)):\n","            unigram_prob = (unigram_counter[sent[i]] + ALPHA) / (C + ALPHA * vocab_size)\n","            prob += math.log2(unigram_prob)\n","    elif ngram == 2:\n","        # Pad sentence depending on the ngram parameter\n","        sent = ['<start>'] + sent + ['<end>']\n","        for i in range(0, len(sent) - (ngram - 1)):\n","            bigram_prob = (bigram_counter[(sent[i], sent[i + 1])] + ALPHA) / (\n","                        unigram_counter[sent[i]] + ALPHA * vocab_size)\n","            prob += math.log2(bigram_prob)\n","    else:\n","        # Pad sentence depending on the ngram parameter\n","        sent = ['<start1>'] + ['<start2>'] + sent + ['<end>']\n","        for i in range(0, len(sent) - (ngram - 1)):\n","            trigram_prob = (trigram_counter[(sent[i], sent[i + 1], sent[i + 2])] + ALPHA) / (\n","                        bigram_counter[(sent[i], sent[i + 1])] + ALPHA * vocab_size)\n","            prob += math.log2(trigram_prob)\n","\n","    return prob"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"PT_xCB-TUo8Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612641743871,"user_tz":-120,"elapsed":574,"user":{"displayName":"Stratos Xenouleas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR90vC-94SBOfz6YtZudAly36OJWSWrSktd543Xg=s64","userId":"04399389374450208758"}},"outputId":"ecec834c-e165-40ee-a2eb-25bb9aeba970"},"source":["# In this part we will compare the log probabilities of some of the sentences in the test dataset\n","# with the one's in random words from the vocab.\n","# We presume that the log probability of the sentences from the test dataset will be much higher\n","# than the random vocab words (of the same length)\n","\n","# Print the probabilities of 5 sentences from the test set\n","\n","print()\n","print('Evalutate Real-Fake Sentences Log Probabilities')\n","for i, sent in enumerate(test_set):\n","    print()\n","    print('Real Sentence {}'.format(i+1))\n","    print(' '.join(sent))\n","    # Create a random sentence from the vocab of the same length\n","    rand_sent = [inv_vocab[random.randint(0, vocab_size-1)] for _ in range(len(sent))]\n","    print()\n","    print('Fake sentence {}'.format(i+1))\n","    print(' '.join(rand_sent))\n","    # Using the unigram language model\n","    prob = predict(sent, 1)\n","    prob2 = predict(rand_sent, 1)\n","    print()\n","    print('Unigram model')\n","    print('Real: {} -- Fake: {}'.format(prob, prob2))\n","    # Using the bigram language model\n","    prob = predict(sent, 2)\n","    prob2 = predict(rand_sent, 2)\n","    print()\n","    print('Bigram model')\n","    print('Real: {} -- Fake: {}'.format(prob, prob2))\n","    # Using the trigram language model\n","    prob = predict(sent, 3)\n","    prob2 = predict(rand_sent, 3)\n","    print()\n","    print('Trigram model')\n","    print('Real: {} -- Fake: {}'.format(prob, prob2))\n","    if i == 2: break"],"execution_count":10,"outputs":[{"output_type":"stream","text":["\n","Evalutate Real-Fake Sentences Log Probabilities\n","\n","Real Sentence 1\n","he would <UNK> himself from <UNK> again such <UNK> <UNK> <UNK> had gone to <UNK> to be indifferent .\n","\n","Fake sentence 1\n","look entirely looking few loved brought delighted her uncle next proof appear absence put feel observe crown completely in\n","\n","Unigram model\n","Real: -123.32987405202242 -- Fake: -219.71561591948517\n","\n","Bigram model\n","Real: -119.77226892195883 -- Fake: -250.720055946711\n","\n","Trigram model\n","Real: -155.29472709170244 -- Fake: -204.26339986733186\n","\n","Real Sentence 2\n","but he had gone to a wrong place .\n","\n","Fake sentence 2\n","pay companion inferior means pity box odd felt therefore\n","\n","Unigram model\n","Real: -70.83922991680878 -- Fake: -111.60699539095229\n","\n","Bigram model\n","Real: -54.96483148523077 -- Fake: -126.25593757402588\n","\n","Trigram model\n","Real: -68.9576437358984 -- Fake: -110.62420038197831\n","\n","Real Sentence 3\n","there was too much <UNK> happiness in his <UNK> house woman <UNK> too amiable a form in it isabella was too much like <UNK> only in those <UNK> <UNK> which always brought the other in <UNK> before him for much to have been done even had his time been <UNK> had <UNK> on however <UNK> day after <UNK> this very <UNK> <UNK> had <UNK> the history of jane <UNK> with the <UNK> which must be felt <UNK> which he did not <UNK> to feel having never believed frank churchill to be at all <UNK> emma was there so much fond <UNK> so much <UNK> anxiety for her that he could stay no longer .\n","\n","Fake sentence 3\n","he towards actually consider consciousness more brother already second likeness belong fortnight help strength recollect air being dinner excellent compliments therefore would sigh compliment staying subject pain <start1> face persuaded write spend likely unless knew side room rain purpose true proved curiosity or <start1> eager told new useful walk mr. understood certain talking met silent company supposed truly begged paid mothers grandmama among companions arrival friendly begged greater dixon into these ready reason yesterday mere attached isabella view agreed anywhere known delightful history likely back value ones middle wanting simple admiration appeared all <start> sea are part looking gentle nonsense stay spent find knightleys passage amusement observation round minutes hundred fairfaxs cheerful house farther\n","\n","Unigram model\n","Real: -851.5644092180667 -- Fake: -1408.190147315755\n","\n","Bigram model\n","Real: -786.9999328789028 -- Fake: -1376.7517693191762\n","\n","Trigram model\n","Real: -936.6490196907052 -- Fake: -1166.8759109509444\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"24M5ZqTQVcFx"},"source":["# Part C"]},{"cell_type":"code","metadata":{"id":"GjAoJ-xiVeKL","executionInfo":{"status":"ok","timestamp":1612641753688,"user_tz":-120,"elapsed":576,"user":{"displayName":"Stratos Xenouleas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR90vC-94SBOfz6YtZudAly36OJWSWrSktd543Xg=s64","userId":"04399389374450208758"}}},"source":["# Method that calculates the cross entropy of a list of sentences\n","# implemented according to https://en.wikipedia.org/wiki/Cross_entropy\n","def cross_entropy(sentences, ngram):\n","    # We must treat the list of sentenes as a big sentence according to the exercise so\n","    # loop through all the sentences and aggregate the probabilities and the sentences\n","    prob = 0\n","    size = 0\n","    for sent in sentences:\n","        prob += predict(sent, ngram)\n","        size += len(sent) + 1 if ngram != 1 else len(sent)\n","\n","    # Divide by the size to get the cross entropy estimation\n","    return -(prob / size)\n","\n","# Method that calculates the perplexity for a list of sentences using the language models\n","def perplexity(sentences, ngram):\n","    perplexity = 2 ** cross_entropy(sentences, ngram)\n","    return perplexity"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"-vuxg5piVqB4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612641754223,"user_tz":-120,"elapsed":664,"user":{"displayName":"Stratos Xenouleas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR90vC-94SBOfz6YtZudAly36OJWSWrSktd543Xg=s64","userId":"04399389374450208758"}},"outputId":"963c7f71-b47f-4ecc-f1e6-550a8a1b829a"},"source":["print(\"2-gram model language cross-entropy : \", cross_entropy(test_set, 2))\n","print(\"2-gram model language perplexity : \", perplexity(test_set, 2))\n","print()\n","print(\"3-gram model language cross-entropy : \", cross_entropy(test_set, 3))\n","print(\"3-gram model language perplexity : \", perplexity(test_set, 3))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["2-gram model language cross-entropy :  6.259240843606695\n","2-gram model language perplexity :  76.59831990811904\n","\n","3-gram model language cross-entropy :  7.562849465119887\n","3-gram model language perplexity :  189.07954201515824\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K16T4meoVzD3"},"source":["# Part D"]},{"cell_type":"code","metadata":{"id":"c8nC-iNtV0tn","executionInfo":{"status":"ok","timestamp":1612642826871,"user_tz":-120,"elapsed":757,"user":{"displayName":"Stratos Xenouleas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR90vC-94SBOfz6YtZudAly36OJWSWrSktd543Xg=s64","userId":"04399389374450208758"}}},"source":["# Use linear interpolation to combine the predictions of the 2 language models\n","def linear_predict(sent, lam=[0.5 , 0.5]):\n","    # Pad sentence\n","    sentence = ['<start1>'] + ['<start2>'] + sent + ['<end>']\n","\n","    # Find the aggragated log probability of the sentence using linear interpolation of the\n","    # trigram and bigram probabilities for each word\n","    prob = 0\n","    for i in range(0, len(sentence) - 2):\n","        trigram_prob = (trigram_counter[(sentence[i], sentence[i + 1], sentence[i + 2])] + ALPHA) / (\n","                    bigram_counter[(sentence[i], sentence[i + 1])] + ALPHA * vocab_size)\n","    \n","        bigram_prob = (bigram_counter[(sentence[i+1], sentence[i + 2])] + ALPHA) / (\n","                    unigram_counter[sentence[i+1]] + ALPHA * vocab_size)\n","\n","        # Linear interpolation of probabilities\n","        prob += math.log2(lam[0] * bigram_prob + lam[1] * trigram_prob)\n","\n","        # print('P({}| {}, {})'.format(sentence[i+2],sentence[i], sentence[i+1]))\n","        # print('P({}| {})'.format(sentence[i+1], sentence[i]))\n","\n","    return prob\n","    \n","# Cross entropy using linear interpolation of language models\n","def linear_cross_entropy(sentences, lam=[0.5, 0.5]):\n","    # We must treat the list of sentenes as a big sentence according to the exercise so\n","    # loop through all the sentences and aggregate the probabilities and the sentences\n","    prob = 0\n","    size = 0\n","    for sent in sentences:\n","        prob += linear_predict(sent, lam)\n","        size += len(sent) + 1\n","    # Divide by the size to get the cross entropy estimation\n","    return -(prob / size)\n","\n","# Perplexity using linear interpolation of language models\n","def linear_perplexity(sentences, lam=[0.5, 0.5]):\n","    perplexity = 2 ** linear_cross_entropy(sentences, lam)\n","    return perplexity"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"RPYbiH29WDZN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612642830410,"user_tz":-120,"elapsed":610,"user":{"displayName":"Stratos Xenouleas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiR90vC-94SBOfz6YtZudAly36OJWSWrSktd543Xg=s64","userId":"04399389374450208758"}},"outputId":"29a6d9d3-4faa-46c0-e48f-15a58a802016"},"source":["print(\"2-gram model language cross-entropy : \", cross_entropy(test_set, 2))\n","print(\"2-gram model language perplexity : \", perplexity(test_set, 2))\n","print()\n","print(\"3-gram model language cross-entropy : \", cross_entropy(test_set, 3))\n","print(\"3-gram model language perplexity : \", perplexity(test_set, 3))\n","print()\n","print(\"linear model language cross-entropy : \", linear_cross_entropy(test_set))\n","print(\"linear model language perplexity : \", linear_perplexity(test_set))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["2-gram model language cross-entropy :  6.259240843606695\n","2-gram model language perplexity :  76.59831990811904\n","\n","3-gram model language cross-entropy :  7.562849465119887\n","3-gram model language perplexity :  189.07954201515824\n","\n","linear model language cross-entropy :  5.839592381453616\n","linear model language perplexity :  57.265422501252935\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T2Spt1MqWIzh"},"source":["# Method that fine-tunes the lambda parameters for the linear interpolation for a given set of sentences\n","def train_linear(sentences, step):\n","    best = float('inf')\n","    best_lam = [0.5, 0.5]\n","    vals = np.arange(0, 1, step)\n","    for v in tqdm(vals):\n","        lamdas = [v, 1-v]\n","        cross = linear_cross_entropy(sentences, lam=lamdas)\n","        if cross < best:\n","            best = cross\n","            best_lam = lamdas\n","    return best_lam"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uzFcwhTBWTUI","colab":{"base_uri":"https://localhost:8080/","height":706},"executionInfo":{"status":"ok","timestamp":1583684015910,"user_tz":-120,"elapsed":59224,"user":{"displayName":"Lefteris Harteros","photoUrl":"","userId":"03384468474275871690"}},"outputId":"09301942-6cad-4d40-b3a6-b233ecb64670"},"source":["# Find the best lambdas\n","lamdas = train_linear(val_set, 0.001)\n","\n","print()\n","print(\"Best lamdas : \", lamdas)\n","\n","# Get the log probability on test set using the default lambdas and then the tuned lambdas\n","for i, sent in enumerate(test_set):\n","    print()\n","    print('Sentence {}'.format(i+1))\n","    print(' '.join(sent))\n","    print()\n","    print('Default lambdas log probability')\n","    prob = linear_predict(sent)\n","    print(prob)\n","    print(\"----------\")\n","    print('Tuned lambdas log probability')\n","    prob = linear_predict(sent, lam=lamdas)\n","    print(prob)\n","    print(\"----------\")\n","    if i == 2: break\n","\n","print()\n","print(\"default linear model language cross-etropy : \", linear_cross_entropy(test_set))\n","print(\"default linear model language perplexity : \", linear_perplexity(test_set))\n","print()\n","print(\"tuned linear model language cross-etropy : \", linear_cross_entropy(test_set, lam=lamdas))\n","print(\"tuned linear model language perplexity : \", linear_perplexity(test_set, lam=lamdas))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 1000/1000 [00:54<00:00, 17.76it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Best lamdas :  [0.774, 0.22599999999999998]\n","\n","Sentence 1\n","he would <UNK> himself from <UNK> again such <UNK> <UNK> <UNK> had gone to <UNK> to be indifferent .\n","\n","Default lambdas log probability\n","-116.48621045502652\n","----------\n","Tuned lambdas log probability\n","-112.29302895024749\n","----------\n","\n","Sentence 2\n","but he had gone to a wrong place .\n","\n","Default lambdas log probability\n","-48.734729206985946\n","----------\n","Tuned lambdas log probability\n","-46.64366928326629\n","----------\n","\n","Sentence 3\n","there was too much <UNK> happiness in his <UNK> house woman <UNK> too amiable a form in it isabella was too much like <UNK> only in those <UNK> <UNK> which always brought the other in <UNK> before him for much to have been done even had his time been <UNK> had <UNK> on however <UNK> day after <UNK> this very <UNK> <UNK> had <UNK> the history of jane <UNK> with the <UNK> which must be felt <UNK> which he did not <UNK> to feel having never believed frank churchill to be at all <UNK> emma was there so much fond <UNK> so much <UNK> anxiety for her that he could stay no longer .\n","\n","Default lambdas log probability\n","-766.9371003195782\n","----------\n","Tuned lambdas log probability\n","-757.513776478326\n","----------\n","\n","default linear model language cross-etropy :  5.839592381453616\n","default linear model language perplexity :  57.265422501252935\n","\n","tuned linear model language cross-etropy :  5.747359026897324\n","tuned linear model language perplexity :  53.718943526255394\n"],"name":"stdout"}]}]}